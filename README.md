# Run a batch:

```sh
POOLS="d2sv3 d2v3 ds2v2 ds3v2"

for POOL in $POOLS; do
    helm template . --set name=speedtest-$POOL --set poolName=$POOL | kubectl apply -f -
done

PODS=$(kubectl get pods -o json | jq ".items[].metadata.name" -r)
for POD in $PODS; do
    echo
    echo "# On host ($POD)"
    echo
    kubectl logs $POD
done
```

# Run on a given pool:

```sh
helm template . --set name=iops-d2sv3-2 --set poolName=agentpool | kubectl apply -f -
```

Test disk IO on kubernetes.

The solution is built in C# / .net core in src. Build the docker image using the Dockerfile in the `src` folder.

`provision.sh` provisions a storage account and a couple of disks to run the tests against.

Then the job is triggered through a helm chart. (source the environment file generated by the provisioning script first).

Examples:

- Run using virtual nodes: `helm template . --set name=speedtest-aci --set virtualnodes=true | kubectl apply -f -`
- Run with fileshare
    - standard: `helm template . --set name=speedtest-fileshare-standard --set shareName=$SHARE_NAME --set accountName=$STANDARDSTORAGE_STORAGE_ACCOUNT --set accountKey=$STANDARD_STORAGE_ACCOUNT_KEY --set testfile=/storage/folder --set storageName=standard --set poolName=agentpool | kubectl apply -f -`
    - premium `helm template . --set name=speedtest-fileshare-premium --set shareName=$SHARE_NAME --set accountName=$PREMIUMSTORAGE_STORAGE_ACCOUNT --set accountKey=$PREMIUM_STORAGE_ACCOUNT_KEY --set testfile=/storage/test --set storageName=premium  --set poolName=agentpool | kubectl apply -f -`
- Run with disk
    - Standard `helm template . --set name=speedtest-disk-standard --set diskId=$STANDARDDISK_DISK_RESOURCE_ID --set diskName=$STANDARDDISK_DISK --set testfile=/storage/test --set poolName=agentpool | kubectl apply -f -`
    - Premium `helm template . --set name=speedtest-disk-premium-2 --set diskId=$PREMIUMDISK_DISK_RESOURCE_ID --set diskName=$PREMIUMDISK_DISK --set testfile=/storage/test --set poolName=agentpool | kubectl apply -f -`

There are a bunch of scripts that let you run all of that:

- `run.sh` is running a single job using determined parameters and grabs the results from the pod. It needs the environment variables, so run it with `source run.sh [OPTIONS]`.
- `run-all-options.sh` is running a set of parameters on all "options": several pools, aci, agent, shares and disks.
- `run-various-parameters.sh` is running all options for files in 10k, 100k, 1M, 10M and 100M.

You also need to [install blob-fuse](https://github.com/Azure/kubernetes-volume-drivers/tree/master/flexvolume/blobfuse) before running it (comment out the blob-fuse tests if you don't wanna).